linear_model = glm(y~x1)
linear_model = lm(y~x1)
summary(linear_model)
linear_model = lm(y~x2)
summary(linear_model)
X=rnorm(100)
Y = 2 + 3*X
lm(Y~X)
rm(lis=ls())
rm(list=ls())
X = runif(100,0,1)
Y = 2 + 3*X
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = runif(100,0,1)
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = 1:100
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = 1:100
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = 1:100
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = 1:100
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
X = 1:100
e = rnorm(100,0,1.5)
Y = 2 + 3*X + e
lm(Y~X)
library(ISLR)
data = Auto
plot(data$mpg,data$horsepower)
#non linear so have to apply transformation
plot(log(data$mpg),data$horsepower)
#non linear so have to apply transformation
plot(y = log(data$mpg),x= data$horsepower)
# ST 4035
# Logistic Regression using lasso
# ------------------------------------
#load required library
library(mlbench)
#load Pima Indian Diabetes dataset
data("PimaIndiansDiabetes")
#set seed to ensure reproducible results
set.seed(42)
#split into training and test sets
PimaIndiansDiabetes[,"train"] = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
#separate training and test sets
trainset = PimaIndiansDiabetes[PimaIndiansDiabetes$train==1,]
testset = PimaIndiansDiabetes[PimaIndiansDiabetes$train==0,]
#get column index of train flag
trainColNum = grep("train",names(trainset))
#remove train flag column from train and test sets
trainset = trainset[,-trainColNum]
testset = testset[,-trainColNum]
#get column index of response variable in dataset
typeColNum = grep("diabetes",names(PimaIndiansDiabetes))
#build model
glm_model <- glm(diabetes~.,data = trainset, family = binomial)
summary(glm_model)
#predict probabilities on testset
#type="response" gives probabilities, type="class" gives class
glm_prob = predict.glm(glm_model,
testset[,-typeColNum],type="response")
#which classes do these probabilities refer to? What are 1 and 0?
contrasts(PimaIndiansDiabetes$diabetes)
#make predictions
##.first create vector to hold predictions
glm_predict = rep("neg",nrow(testset))
glm_predict[glm_prob>.5] = "pos"
#confusion matrix
table(pred=glm_predict,true=testset$diabetes)
#accuracy
mean(glm_predict==testset$diabetes)
source("D:/University/4th Year/ST4035 - Data Science/Lecture_Excercises/Supervised Machine Learning/Supervised ML.R")
version()
version
#Practice for the assesment
library(ISLR)
version
#Practice for the assesment
library(ISLR)
rm(list=ls())
data("Hitters")
force(Hitters)
myHitters =na.omit(Hitters)
duplicated(myHitters)
#Practice for the assesment
library(ISLR)
data("Hitters")
myHitters =na.omit(Hitters)
x = model.matrix(Salary~.,myHitters )[,-1]
y = myHitters$Salary
#Train Test Split
ridge.mod = glmnet(x,y,alpha = 0)
library(glmnet)
install.packages('glmnet')
install.packages('tidyverse')
#Train Test Split
ridge.mod = glmnet(x,y,alpha = 0)
source("~/.active-rstudio-document")
#Practice for the assesment
library(ISLR)
library(glmnet)
data("Hitters")
myHitters =na.omit(Hitters)
x = model.matrix(Salary~.,myHitters )[,-1]
y = myHitters$Salary
#Train Test Split
ridge.mod = glmnet(x,y,alpha = 0)
ridge.mod
#Setting a grid matrix for lambda
grid = 10^seq(10,-2,length=100)
ridge.mod = glmnet(x,y,alpha = 0, lambda=grid)
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
predict(ridge.mod, s=50,  type='coefficient')[1:20,]
#Splitting the dataset
set.seed(1)
#Splitting the dataset
set.seed(1)
train = sample(1:nrow(x),  nrow(x)/2)
test = -train
test = (-train)
y.test = y[test]
#Tesing ridge at lambda 4
ridge.mod = glmnet(x[train,],y[train],alpha = 0, lambda=grid)
#Tesing ridge at lambda 4
ridge.mod = glmnet(x[train,],y[train],alpha = 0, lambda=grid, thresh=1e^12)
#Tesing ridge at lambda 4
ridge.mod = glmnet(x[train,],y[train],alpha = 0, lambda=grid, thresh=1e-12)
mean((ridge.pred - y.test)^2)
mean((ridge.pred-y.test)^2)
mean((ridge.pred-y.test)**2)
ridge.pred = predict(ridge.mod,s=4,newx = x[test,])
View(ridge.pred)
mean((ridge.pred-y.test)**2)
#Cross Validation
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)**2)
#Finally fitting the entire dataset
out = glmnet (x,y,alpha =0)
predict(out, type="coefficients",s=bestlam )[1:20 ,]
#Lasso (Best for Variable Selection)
lasso.mod = glmnet(x[train],y[train],alpha=1,lambda = grid)
#Lasso (Best for Variable Selection)
lasso.mod = glmnet(x[train,],y[train],alpha=1,lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test ,])
mean((lasso.pred-y.test)^2)
return(standardized.x)
minmax = function(x){
x.min = min(x)
x.max = max(x)
standardized.x = (x - x.min)/(x.max - x.min)
return(standardized.x)
}
out = glment(x,y,alpha=1,lambda=grid)
out = glmnett(x,y,alpha=1,lambda=grid)
out = glmnet(x,y,alpha=1,lambda=grid)
lasso.coef =  predict(out, type='coefficients', s=bestlam)
lasso.coef
rm(list = ls())
clc
library(glmnet)
install.packages('mlbench')
library(mlbench)
data("PimaIndiansDiabetes")
str(PimaIndiansDiabetes)
#set seed to ensure reproducible results
set.seed(42)
#split into training and test sets
PimaIndiansDiabetes[,"train"] = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
rm(PimaIndiansDiabetes)
data("PimaIndiansDiabetes")
str(PimaIndiansDiabetes)
#set seed to ensure reproducible results
set.seed(42)
#split into training and test sets
PimaIndiansDiabetes$train = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
trainset = PimaIndiansDiabetes[PimaIndiansDiabetes$train==1]
trainset = PimaIndiansDiabetes[,PimaIndiansDiabetes$train==1]
trainset = PimaIndiansDiabetes[PimaIndiansDiabetes$train==1,]
#split into training and test sets
train_index = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
trainset = PimaIndiansDiabetes[PimaIndiansDiabetes$train==1,]
trainset = PimaIndiansDiabetes[PimaIndiansDiabetes$train==1,]
trainset = PimaIndiansDiabetes[train_index==1,]
testset = PimaIndiansDiabetes[train_index==0,]
#
typeColNum = grep("diabetes",names(PimaIndiansDiabetes))
glm_model = glm(diabetes~.,data = testset, family = binomial)
summary(glm_model)
library(glmnet)
library(mlbench)
data("PimaIndiansDiabetes")
str(PimaIndiansDiabetes)
#set seed to ensure reproducible results
set.seed(42)
#split into training and test sets
train_index = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
trainset = PimaIndiansDiabetes[train_index==1,]
testset = PimaIndiansDiabetes[train_index==0,]
#Getting  column index of the response
typeColNum = grep("diabetes",names(PimaIndiansDiabetes))
glm_model = glm(diabetes~.,data = testset, family = binomial)
summary(glm_model)
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="response")
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="class")
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="class")
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="terms")
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="link")
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="response")
glm_prob = ifelse(glm_prob>0.5,1,0)
glm_prob = ifelse(glm_prob>0.5,2,1)
glm_prob = ifelse(glm_prob>0.5,'pos','neg')
#confusion matrix
table(pred=glm_predict,true=testset$diabetes)
#confusion matrix
table(pred=glm_prob,true=testset$diabetes)
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="response")
library(glmnet)
library(mlbench)
data("PimaIndiansDiabetes")
str(PimaIndiansDiabetes)
#set seed to ensure reproducible results
set.seed(42)
#split into training and test sets
train_index = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
trainset = PimaIndiansDiabetes[train_index==1,]
testset = PimaIndiansDiabetes[train_index==0,]
#Getting  column index of the response
typeColNum = grep("diabetes",names(PimaIndiansDiabetes))
glm_model = glm(diabetes~.,data = testset, family = binomial)
summary(glm_model)
#type="response" gives probabilities, type="class" gives class
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="response")
glm_predict = rep("neg",nrow(testset))
glm_predict[glm_prob>.5] = "pos"
#confusion matrix
table(pred=glm_prob,true=testset$diabetes)
#which classes do these probabilities refer to? What are 1 and 0?
contrasts(PimaIndiansDiabetes$diabetes)
library(glmnet)
library(mlbench)
data("PimaIndiansDiabetes")
str(PimaIndiansDiabetes)
#set seed to ensure reproducible results
set.seed(42)
#split into training and test sets
train_index = ifelse(
runif(nrow(PimaIndiansDiabetes))<0.8,1,0
)
trainset = PimaIndiansDiabetes[train_index==1,]
testset = PimaIndiansDiabetes[train_index==0,]
#Getting  column index of the response
typeColNum = grep("diabetes",names(PimaIndiansDiabetes))
glm_model = glm(diabetes~.,data = testset, family = binomial)
summary(glm_model)
#type="response" gives probabilities, type="class" gives class
glm_prob = predict.glm(glm_model,testset[,-typeColNum],type="response")
#which classes do these probabilities refer to? What are 1 and 0?
contrasts(PimaIndiansDiabetes$diabetes)
#accuracy
mean(glm_predict==testset$diabetes)
#Lasso with ridge
x =model.matrix(diabetes~.,train)
#Lasso with ridge
x =model.matrix(diabetes~.,trainset)
y = ifelse(trainset$diabetes=='pos',1,0)
cv.out = cv.glmnet(x,y,alpha=1,family = 'binomial',type.measure = 'mse')
#cv.out with lasso
cv.out = cv.glmnet(x,y,alpha=1,family = 'binomial',type.measure = 'mse')
plot(cv.out)
x_test = model.matrix(diabetes~.,testset)
library(tidyverse)
rm(list=ls())
data("iris")
force(iris)
str(iris)
iris = distinct(iris)
iris |> group_by(Species) |> summarise()
iris |> group_by(Species) |> summarise(avg=mean(Sepal.Length))
time=c(2,4,7,8,5)
library('survival')
time=c(2,4,7,8,5)
censoor=c(1,1,1,1,1) #Have to give censoring indicator
tt1=Surv(time,censoor)
tt1
time=c(2,4,7,8,5)
censoor=c(1,1,0,1,0) #Have to give censoring indicator
tt1=Surv(time,censoor)
#Getting cummulative probs
survfit(tt1~1)
#Getting survival probabilities
surv_estimates = survfit(tt1~1)
summary(surv_estimates)
time=c(2,4,7,8,5)
censoor=c(1,1,1,1,1) #Have to give censoring indicator
tt1=Surv(time,censoor)
#Getting survival probabilities
surv_estimates = survfit(tt1~1)
summary(surv_estimates)
y=c(2,3,6,7,8,9,10,12,15)
glm1=glm(y~x,family = 'poisson')
x=c(-1,-1,0,0,0,0,1,1,1)
glm1=glm(y~x,family = 'poisson')
summary(glm1)
qchisq(0.05,3)
qchisq(0.95,7)
qchisq(0.05,7,lower.tail = FALSE)
library(ISLR)
rm(list=ls())
library(ISLR)
data("Hitters")
head(Hitters)
dim(Hitters)
names(Hitters)
library(rpart)
library(rpart.plot)
installed.packages('rpart.plot')
install.packages('rpart.plot')
library(rpart.plot)
baseTree = rpart(Salary ~ Years + Hits, data=Hitters)
rpart.plot(baseTree, type = 1)
baseTree$variable.importance
# Pruning
set.seed(1984)
train = sample(1:nrow(hitters), nrow(hitters)/2)
train = sample(1:nrow(Hitters), nrow(Hitters)/2)
tree.baseball = rpart(Salary ~ Hits + HmRun + Runs + RBI + Walks + Years + Errors,
subset = train, data = Hitters)
testData = Hitters[-train,]
summary(tree.baseball)
rpart.plot(tree.baseball, type = 1)
rpart.plot(tree.baseball, type = 1)
tree.baseball$variable.importance
printcp(tree.baseball)
plotcp(tree.baseball)
#optimal CP = 0.018
tree.pruned = prune(tree.baseball, cp=0.018)
rpart.plot(tree.pruned)
predSalary = predict(tree.pruned,testData)
data(iris)
rm(list=ls())
data(iris)
force(iris)
library(e1071)
install.packages('rethnicity')
library(rethnicity)
predict_ethnicity("Ch'ang")
predict_ethnicity(lastnames = "Ch'ang")
predict_ethnicity(firstnames = "A", lastnames = "Ch'ang")
predict_ethnicity(firstnames = "A", lastnames = "Chukwuebuka")
predict_ethnicity(firstnames = "A", lastnames = "Genovese")
predict_ethnicity(firstnames = "A", lastnames = "Chukwuemeka")
installed.packages('neuralnet')
install.packages('neuralnet')
library(neuralnet)
library(tidyverse)
data("iris")
force(iris)
View(iris)
str(iris)
iris = iris |> mutate_if(is.character, as.factor)
set.seed(245)
data_rows = floor(0.8*nrow(iris))
train = sample(iris,data_rows)
train_inidces = sample(nrow(iris),data_rows)
train = iris[train_inidces]
train = iris[train_inidces,]
test = iris[-train_inidces,]
#Neural Network Model
attach(train)
model = neuralnet(
data = train,
Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width
)
model = neuralnet(
data = train,
Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
hidden=c(4,2),
linear.output = FALSE
)
plot(model,rep = 'best')
preds = predict(model,test)
preds
test = iris[-train_inidces,1:4]
library(neuralnet)
library(tidyverse)
data("iris")
str(iris)
iris = iris |> mutate_if(is.character, as.factor)
set.seed(245)
data_rows = floor(0.8*nrow(iris))
train_inidces = sample(nrow(iris),data_rows)
train = iris[train_inidces,]
test = iris[-train_inidces,1:4]
#Neural Network Model
attach(train)
model = neuralnet(
data = train,
Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
hidden=c(4,2),
linear.output = FALSE
)
plot(model,rep = 'best')
preds = predict(model,test)
preds
View(preds)
View(preds)
library(neuralnet)
library(tidyverse)
data("iris")
str(iris)
iris = iris |> mutate_if(is.character, as.factor)
set.seed(245)
data_rows = floor(0.8*nrow(iris))
train_inidces = sample(nrow(iris),data_rows)
train = iris[train_inidces,]
test = iris[-train_inidces,]
#Neural Network Model
attach(train)
model = neuralnet(
data = train,
Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
hidden=c(4,2),
linear.output = FALSE
)
plot(model,rep = 'best')
preds = predict(model,test)
preds
library(neuralnet)
library(tidyverse)
data("iris")
str(iris)
iris = iris |> mutate_if(is.character, as.factor)
set.seed(245)
data_rows = floor(0.8*nrow(iris))
train_inidces = sample(nrow(iris),data_rows)
train = iris[train_inidces,]
test = iris[-train_inidces,]
#Neural Network Model
attach(train)
model = neuralnet(
data = train,
Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
hidden=c(4,2,3),
linear.output = FALSE
)
plot(model,rep = 'best')
preds = predict(model,test)
preds
View(preds)
preds[1,2]
preds[1,]
summary(model)
library(caret)
confusionMatrix(data = preds, reference = test$Species)
