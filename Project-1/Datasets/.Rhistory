X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
test_set$Final=y_test
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
anova(model, test="Chisq")
library(psci)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
install.packages("psci")
library(psci)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
install.packages("pscl")
library(pscl)
#Checking the fit of the model
pR2(model)
str(train_set)
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
#anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
#anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
#anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
#anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
set.seed(100)
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
#anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
newwd="D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets"
setwd(newwd)
#install.packages('dplyr', repos = 'https://cloud.r-project.org')
#install.packages("caTools")
#install.packages("ROCR")
#install.packages("pscl")
set.seed(100)
library(dplyr)
library(caTools)
library(ROCR)
library(pscl)
train=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/train_main.csv")
test=read.csv("D:/University/4th Year/ST4035 - Data Science/University-Data-Science-ST4035-Projects/Project-1/Datasets/test_main.csv")
y = train$Final
y = y - 1
X_train = train %>% select(-Final)
head(train)
summary(X_train)
str(X_train)
count_unique = function(col) {
return(length(unique(col)))
}
#Categoricas being weeded out
unique_counts = apply(X_train, 2, count_unique)
float_indexes=unique_counts>15
cat_cols=colnames(X_train)[!float_indexes]
X_train[cat_cols] = lapply(X_train[cat_cols], factor)
test[cat_cols] = lapply(test[cat_cols], factor)
#Train test split
ratio = 0.8
train_indices = sample(1:nrow(X_train), size = round(nrow(X_train) * ratio))
# Split the data into training and test sets
train_set <- X_train[train_indices, ]
test_set <- X_train[-train_indices, ]
y_train = y[train_indices]
y_test = y[-train_indices]
train_set$Final=y_train
str(train_set)
model = glm(Final~.,family = binomial, data = train_set)
summary(model)
#Checking for deviance
#anova(model, test="Chisq")
predictions = predict(model, newdata = test_set, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
#Checking for deviance
anova(model, test="Chisq")
#Checking for deviance
ANOVA_MODEL_1=anova(model, test="Chisq")
#New Classification
ANOVA_MODEL_1$`Pr(>Chi)` < 0.05
#New Classification
(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)[2:nrows(ANOVA_MODEL_1)]
#New Classification
(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)[2:length(ANOVA_MODEL_1$`Pr(>Chi)`)]
#New Classification
cols_for_backward_selection=(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)[2:length(ANOVA_MODEL_1$`Pr(>Chi)`)]
#New Classification
cols_for_backward_selection=(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)
#New Classification
cols_for_backward_selection=colnames(train_set[(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)])
#New Classification
cols_for_backward_selection=(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)
colnames(test_set)[cols_for_backward_selection]
cols_for_backward_selection=lapply(cols_for_backward_selection, replace_na_with_true)
replace_na_with_true = function(x) {
is_na = is.na(x)
x[is_na]=TRUE
return(x)
}
cols_for_backward_selection=lapply(cols_for_backward_selection, replace_na_with_true)
colnames(train_set)[cols_for_backward_selection]
#New Classification
cols_for_backward_selection=(ANOVA_MODEL_1$`Pr(>Chi)` < 0.05)
replace_na_with_false = function(x) {
is_na = is.na(x)
x[is_na]=FALSE
return(x)
}
cols_for_backward_selection=replace_na_with_false(cols_for_backward_selection)
colnames(train_set)[cols_for_backward_selection]
replace_na_with_false = function(x) {
is_na = is.na(x)
x[is_na]=FALSE
return(x)
}
replace_na_with_true = function(x) {
is_na = is.na(x)
x[is_na]=TRUE
return(x)
}
cols_for_backward_selection=replace_na_with_true(cols_for_backward_selection)
colnames(train_set)[cols_for_backward_selection]
colnames(train_set)[cols_for_backward_selection[2:137]]
colnames(train_set)[cols_for_backward_selection[2:136]]
colnames(train_set)[cols_for_backward_selection[2:135]]
colnames(test_set)[cols_for_backward_selection[2:135]]
back_cols_1=colnames(test_set)[cols_for_backward_selection[2:135]]
#2nd iteration
model = glm(Final~back_cols_1,family = binomial, data = train_set)
#2nd iteration
model = glm(Final~"Year"+"Month"+"Hospital"+"Education"+"Possibleexposure"+"SOBonset"+"Skinrashad"+"Neckstiffnessad"+"Fever2"+"Chills4"+"Vomiting2"+"Anuria3"+"Confusion2"+"SOB2"+"WPqPCRDiagnosis"+"Isolate",family = binomial, data = train_set)
#2nd iteration
model = glm(Final~train_set$Year+train_set$Month+train_set$Hospital+train_set$Education+train_set$Possibleexposure+train_set$SOBonset+train_set$Skinrashad+train_set$Neckstiffnessad+train_set$Fever2+train_set$Chills4+train_set$Vomiting2+train_set$Anuria3+train_set$Confusion2+train_set$SOB2+train_set$WPqPCRDiagnosis+train_set$Isolate,family = binomial, data = train_set)
summary(model)
#Checking for deviance
anova(model, test="Chisq")
test_set[back_cols_1]
test_set[back_cols_1,]
test_set[,back_cols_1]
predictions = predict(model, newdata = test_set[,back_cols_1], type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
test_2=test_set[,back_cols_1]
colnames(test)
colnames(test_2)
predictions = predict(model, newdata = test_2, type='response')
back_cols_1=colnames(test_set)[cols_for_backward_selection[2:137]]
back_cols_1
back_cols_1=colnames(test_set)[cols_for_backward_selection[2:137]]
back_cols_1
back_cols_1+"Final"
#2nd iteration
train_set_2=train_set_2[,cols_for_backward_selection]
#2nd iteration
train_set_2=train_set[,cols_for_backward_selection]
#2nd iteration
train_set_2=train_set[,cols_for_backward_selection[1:137]]
#2nd iteration
train_set_2=train_set[,cols_for_backward_selection[1:136]]
#2nd iteration
train_set_2=train_set[,cols_for_backward_selection[1:137]]
#2nd iteration
train_set_2=train_set[,cols_for_backward_selection[2:137]]
test_set_2=test_set[,cols_for_backward_selection[2:136]]
#MODEL
model = glm(Final~.,family = binomial, data = train_set_2)
summary(model)
predictions = predict(model, newdata = test_set_2, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
colnames(test)
colnames(test[,cols_for_backward_selection[2:136]])
test_set_main=test[,cols_for_backward_selection[2:136]])
test_set_main=test[,cols_for_backward_selection[2:136]]
preds = predict(model, newdata = test_set_main, type='response')
preds
preds = ifelse(preds > 0.5,1,0)
preds
preds = preds + 1
data.frame(
"ID"=1:length(preds),
"Final"=preds
)
sub3=data.frame(
"ID"=1:length(preds),
"Final"=preds
)
write.csv(sub3, file = "sub3.csv", row.names = FALSE)
#Checking for deviance
anova(model, test="Chisq")
#Checking for deviance
ANOVA_2=anova(model, test="Chisq")
#Checking for deviance
ANOVA_MODEL_2=anova(model, test="Chisq")
cols_for_backward_selection=(ANOVA_MODEL_2$`Pr(>Chi)` < 0.05)
back_cols_1=colnames(test_set_2)[cols_for_backward_selection[2:137]]
back_cols_1=colnames(test_set_2)[cols_for_backward_selection[2:length(cols_for_backward_selection)]]
train_set_3=train_set[,cols_for_backward_selection[2:length(cols_for_backward_selection)]]
train_set_3=train_set_2[,cols_for_backward_selection[2:length(cols_for_backward_selection)]]
test_set_3=test_set_2[,cols_for_backward_selection[2:length(cols_for_backward_selection)-1]]
test_set_3=test_set_2[,cols_for_backward_selection[2:length(cols_for_backward_selection)]]
#MODEL
model = glm(Final~.,family = binomial, data = train_set_3)
summary(model)
#Checking for deviance
ANOVA_MODEL_2=anova(model, test="Chisq")
predictions = predict(model, newdata = test_set_3, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
test_set_main=test[,cols_for_backward_selection[2:length(cols_for_backward_selection)]]
preds = predict(model, newdata = test_set_main, type='response')
#Main inference
test_set_main=test[,cols_for_backward_selection[2:length(cols_for_backward_selection)]]
#MODEL
model = glm(Final~.,family = binomial, data = train_set_3)
summary(model)
#Checking for deviance
ANOVA_MODEL_2=anova(model, test="Chisq")
predictions = predict(model, newdata = test_set_3, type='response')
predictions = ifelse(predictions > 0.5,1,0)
#Checking for misclassification
misClasificError <- mean(predictions != y_test)
print(paste('Accuracy',1-misClasificError))
#Main inference
test_set_main=test[,cols_for_backward_selection[2:length(cols_for_backward_selection)]]
preds = predict(model, newdata = test_set_main, type='response')
preds = ifelse(preds > 0.5,1,0)
